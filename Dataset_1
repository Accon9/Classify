'''
    自定义数据集

    图像数据 ➡ 图像索引文件 ➡ 使用Dataset构建数据集 ➡ 使用DataLoader读取数据
    步骤：1、图像数据本地存储
         2、索引文件制作，记录图像的文件名和标签信息。
         3、将图像按照训练集和测试集分别存放
         4、写一个的子类来定义数据集
         5、构建训练集和测试集 train_dataset test_dataset
         6、使用DataLoader批量读取数据

    @tips：__getitem__() 函数，该函数接收一个index，也就是索引值。只要是具有索引的数据类型都能够被读取，如list，Series，Dataframe等形式。
            为了方便，我们一般采用list形式将文件代入函数中，该list中的每一个元素包含了图片的路径或标签等信息，以方便index用来逐一读取单一样本数据。
            在__getitem__() 函数内部，我们可以选择性的对图像和标签进行预处理等操作，最后返回图像数据和标签。
'''

from torchvision import transforms, utils
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os

#数据集的设置
root =os.getcwd()+ '/data1/'#调用图像

a = 0
while (a < 1024):  # 1024为我们的类别数
    dir = './data1/images/'  # 图片文件的地址
    label = a
    # os.listdir的结果就是一个list集，可以使用list的sort方法来排序。如果文件名中有数字，就用数字的排序
    files = os.listdir(dir)  # 列出dirname下的目录和文件
    files.sort()  # 排序
    train = open('./data1/train.txt', 'a')
text = open('./data1/text.txt', 'a')
i = 1
for file in files:
    if i < 200000:
        fileType = os.path.split(file)  # os.path.split()：按照路径将文件名和路径分割开
        if fileType[1] == '.txt':
            continue
        name = str(dir) + file + ' ' + str(int(label)) + '\n'
        train.write(name)
        i = i + 1

    else:
        fileType = os.path.split(file)
        if fileType[1] == '.txt':
            continue
        name = str(dir) + file + ' ' + str(int(label)) + '\n'
        text.write(name)
        i = i + 1
text.close()
train.close()

#首先继承上面的dataset类。然后在__init__()方法中得到图像的路径，然后将图像路径组成一个数组，这样在__getitim__()中就可以直接读取：


# 定义读取文件的格式
def default_loader(path):
    return Image.open(path).convert('RGB')


class MyDataset(Dataset):
    # 创建自己的类： MyDataset,这个类是继承的torch.utils.data.Dataset
    # **********************************  #使用__init__()初始化一些需要传入的参数及数据集的调用**********************
    def __init__(self, txt, transform=None, target_transform=None, loader=default_loader):
        super(MyDataset, self).__init__()
        # 对继承自父类的属性进行初始化
        fh = open(txt, 'r')
        imgs = []
        # 按照传入的路径和txt文本参数，以只读的方式打开这个文本
        for line in fh:  # 迭代该列表#按行循环txt文本中的内
            line = line.strip('\n')
            line = line.rstrip('\n')
            # 删除 本行string 字符串末尾的指定字符，这个方法的详细介绍自己查询python
            words = line.split()
            # 用split将该行分割成列表  split的默认参数是空格，所以不传递任何参数时分割空格
            imgs.append((words[0], int(words[1])))
            # 把txt里的内容读入imgs列表保存，具体是words几要看txt内容而定
        # 很显然，根据我刚才截图所示txt的内容，words[0]是图片信息，words[1]是lable
        self.imgs = imgs
        self.transform = transform
        self.target_transform = target_transform
        self.loader = loader
        #使用__getitem__()对数据进行预处理并返回想要的信息

def __getitem__(self, index):  # 按照索引读取每个元素的具体内容
    fn, label = self.imgs[index]
    # fn是图片path #fn和label分别获得imgs[index]也即是刚才每行中word[0]和word[1]的信息
    img = self.loader(fn)
    # 按照路径读取图片
    if self.transform is not None:
        img = self.transform(img)   # 数据标签转换为Tensor
    return img, label       # return回哪些内容，那么我们在训练时循环读取每个batch时，就能获得哪些内容

def __len__(self):  #使用__len__()初始化一些需要传入的参数及数据集的调用
    return len(self.imgs)

train_data = MyDataset(txt=root + 'train.txt', transform=transforms.ToTensor())
test_data = MyDataset(txt=root + 'text.txt', transform=transforms.ToTensor())

#然后就是调用DataLoader和刚刚创建的数据集，来创建dataloader，这里提一句，loader的长度是有多少个batch，所以和batch_size有关
train_loader = DataLoader(dataset=train_data, batch_size=6, shuffle=True,num_workers=4)
test_loader = DataLoader(dataset=test_data, batch_size=6, shuffle=False,num_workers=4)
print('num_of_trainData:', len(train_data))
print('num_of_testData:', len(test_data))
